{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "#from google.cloud import storage\n",
    "import os\n",
    "import json\n",
    "import plotly.express as px\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(\"o9_logger\")\n",
    "\n",
    "pd.options.display.max_rows = 25\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.options.display.precision = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions\n",
    "\n",
    "def calculate_mape(target, predicted):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Percentage Error (MAPE) between ytest and ypred.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ytest : array-like\n",
    "        The true values.\n",
    "    ypred : array-like\n",
    "        The predicted values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float: The MAPE between ytest and ypred.\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays\n",
    "    ytest = np.array(target)\n",
    "    ypred = np.array(predicted)\n",
    "    \n",
    "    # Calculate the absolute percentage error for each point\n",
    "    abs_pct_error = np.abs((target - predicted) / target)\n",
    "    \n",
    "    # Handle divide by zero error\n",
    "    abs_pct_error[np.isinf(abs_pct_error)] = 0.0\n",
    "    \n",
    "    # Calculate the mean absolute percentage error\n",
    "    mape = np.mean(abs_pct_error) * 100\n",
    "    \n",
    "    # Calculate the weighted mean absolute percentage error\n",
    "    wmape = np.sum(np.abs(target - predicted))/np.sum(target)\n",
    "    \n",
    "    return mape, wmape\n",
    "\n",
    "def plot_regression_accuracy(target, predicted, max_error=100, max_range=100):\n",
    "    # Calculate the R-squared and MSE, mape metrics\n",
    "    r2 = r2_score(target, predicted)\n",
    "    rmse_accuracy = np.sqrt(np.mean((target - predicted)**2))\n",
    "    mape,wmape = calculate_mape(target, predicted)\n",
    "    \n",
    "    # Create a scatter plot of the target and predicted values\n",
    "    plt.figure(figsize=(12, 8), dpi=200)\n",
    "    errors = [abs(t - p) for t, p in zip(target, predicted)]\n",
    "    filtered_target = []\n",
    "    filtered_predicted = []\n",
    "    for t, p, e in zip(target, predicted, errors):\n",
    "        if e <= max_error and t <= max_range and p <= max_range:\n",
    "            filtered_target.append(t)\n",
    "            filtered_predicted.append(p)\n",
    "    plt.scatter(filtered_target, filtered_predicted, s=4, alpha=0.3)\n",
    "\n",
    "    # Add a line indicating perfect predictions\n",
    "    min_val = min(filtered_target + filtered_predicted)\n",
    "    max_val = max(filtered_target + filtered_predicted)\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "\n",
    "    # Set the axis labels and plot title\n",
    "    plt.xlabel('Target Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Regression Accuracy (R-squared={r2:.2f}, RMSE={rmse_accuracy:.2f},MAPE={mape:.2f}%,WMAPE={wmape:.2f}%)')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Create a histogram of the error distribution\n",
    "    plt.figure(figsize=(8, 6), dpi=200)\n",
    "    filtered_errors = [t - p for t, p in zip(filtered_target, filtered_predicted)]\n",
    "    sns.histplot(filtered_errors, kde=True)\n",
    "    \n",
    "\n",
    "    # Set the axis labels and plot title\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Distribution')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read base data\n",
    "\n",
    "logger.info(\"------ 1. Read data ------\")\n",
    "\n",
    "try:\n",
    "    sales_df = actuals.copy() \n",
    "    price_df = price_data.copy()\n",
    "    itemmaster_df = itemmaster.copy() \n",
    "    dimtime = timemaster.copy() \n",
    "    assortment_df = assortment_data.copy() \n",
    "    current_week = CurrentWeek.copy()\n",
    "    location_master = locationmaster.copy()\n",
    "    itemseasonweek = itemseason_week.copy()\n",
    "\n",
    "except:\n",
    "    sales_df = pd.read_csv(\"new_data/actuals.csv\")\n",
    "    price_df = pd.read_csv(\"new_data/price_data.csv\")\n",
    "    itemmaster_df = pd.read_csv(\"new_data/itemmaster.csv\")\n",
    "    dimtime = pd.read_csv(\"data/timemaster.csv\")\n",
    "    assortment_df = pd.read_csv(\"data/assortment_data.csv\")\n",
    "    current_week = pd.read_csv(\"data/current_week.csv\")\n",
    "    location_master = pd.read_csv(\"data/location_master.csv\")\n",
    "    itemseasonweek = pd.read_csv('data/itemseasonweek.csv')\n",
    "\n",
    "logger.info(\"sales_df head:{}\".format(sales_df.head()))\n",
    "logger.info(\"price_df head:{}\".format(price_df.head()))\n",
    "logger.info(\"itemmaster_df head:{}\".format(itemmaster_df.head()))\n",
    "logger.info(\"assortment_df head:{}\".format(assortment_df.head()))\n",
    "logger.info(\"itemseasonweek head:{}\".format(itemseasonweek.head()))\n",
    "logger.info(\"location_master head:{}\".format(location_master.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:o9_logger:------ 1.5 Define Variables ------\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"------ 1.5 Define Variables ------\")\n",
    "\n",
    "itemcol = 'Item.[Planning Item]'\n",
    "loccol = \"Location.[Location Country]\"\n",
    "timecol = \"Time.[WeekKey]\"\n",
    "actual = \"Actual\"\n",
    "item_season = 'EB_Item_Seasonality.[EB_Item_Seasonality]'\n",
    "is_assorted = 'EB_Assorted'\n",
    "\n",
    "price_start_date = \"Price_Valid_From\"\n",
    "price_end_date = \"Price_Valid_To\"\n",
    "regular_price = \"EB_Future_Display Price\"\n",
    "final_price = \"EB_Future_Final_Price\"\n",
    "mkd_perc = \"markdown_perc\"\n",
    "\n",
    "week_rank = 'weekrank'\n",
    "week_rank_max = 'week_rank_max'\n",
    "week_rank_mkd = 'week_rank_mkd'\n",
    "window = \"window%\"\n",
    "median_window = 'median_values'\n",
    "mfp_level = \"Item.[EB_Dept_Brand_BM]\"\n",
    "\n",
    "versioncol = 'Version.[Version Name]'\n",
    "\n",
    "assortment_start_date = \"EB_AdjStartWeek\"\n",
    "assortment_end_date = \"EB_AdjEndWeek\"\n",
    "\n",
    "markdown_level = [itemcol,loccol]  # the level at which markdown is done and planned\n",
    "\n",
    "input_version = sales_df[versioncol].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:o9_logger:------ 2. Process Input Data ------\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"------ 2. Process Input Data ------\")\n",
    "\n",
    "## process time dim\n",
    "dimtime.dropna(axis=1,inplace=True)\n",
    "\n",
    "## process itemmaster\n",
    "itemmaster_df[is_assorted].fillna(False,inplace=True)\n",
    "\n",
    "## process assortment data\n",
    "assortment_df['EB_NoMarkDown'].fillna(False,inplace=True)\n",
    "assortment_df = assortment_df.dropna()  # drop rows where data is incomplete\n",
    "\n",
    "## get current_week\n",
    "current_week = current_week[timecol].iloc[0]\n",
    "current_week = pd.to_datetime(current_week)\n",
    "\n",
    "## process sales_df\n",
    "# get proper time column \n",
    "sales_df = sales_df.merge(dimtime,on='Time.[Week]',how = 'left')\n",
    "sales_df = sales_df[[itemcol,loccol,timecol,actual]].reset_index(drop=True)\n",
    "sales_df[timecol] = pd.to_datetime(sales_df[timecol], format='%m/%d/%Y %I:%M:%S %p')\n",
    "# filter sales data from 2 years back to 2 weeks back\n",
    "date_2_years_back = current_week + pd.DateOffset(years=-2)\n",
    "date_2_weeks_back = current_week + pd.DateOffset(weeks=-2)\n",
    "sales_df = sales_df[(sales_df[timecol]>= date_2_years_back)&(sales_df[timecol]<= date_2_weeks_back)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "## process price_df (price missing for 7901 combinations or 0.95 %)\n",
    "# convert date columns to datetime\n",
    "price_df[price_start_date] = pd.to_datetime(price_df[price_start_date], format='%m/%d/%Y %I:%M:%S %p')\n",
    "price_df[price_end_date] = pd.to_datetime(price_df[price_end_date], format='%m/%d/%Y %I:%M:%S %p')\n",
    "price_df = price_df[[itemcol,loccol,price_start_date,price_end_date,regular_price,final_price]].reset_index(drop=True)\n",
    "# calculate markdown perc \n",
    "price_df[mkd_perc] = (price_df[regular_price] - price_df[final_price])/price_df[regular_price]\n",
    "# sort the price data\n",
    "price_df = price_df.sort_values(markdown_level+[price_start_date])\n",
    "# reduce price data to only those combinations where sales data is present\n",
    "price_df = price_df.merge(sales_df[markdown_level].drop_duplicates().reset_index(drop=True),on = markdown_level,how='inner')\n",
    "# step 1 fill na\n",
    "price_df.fillna(0,inplace=True)\n",
    "# step 2 consolidate the dates based on markdown\n",
    "price_df = price_df.groupby(markdown_level+[mkd_perc]).agg({price_start_date:'min',price_end_date:'max',regular_price:'mean',final_price:'mean'}).reset_index()\n",
    "# step 3 remove overlap if present between markdown dates\n",
    "def remove_overlapping_dates(group):\n",
    "    for i in range(1, len(group)):\n",
    "        # If there is an overlap between dates of 2 consecutive markdowns\n",
    "        if (group.iloc[i][price_start_date] <= group.iloc[i-1][price_end_date]):  #and (group.iloc[i-1][mkd_perc] > 0)\n",
    "            # Adjust the 'Price_Valid_To' date of the earlier period\n",
    "            group.iloc[i-1, group.columns.get_loc(price_end_date)] = group.iloc[i][price_start_date] - pd.Timedelta(days=7)\n",
    "    return group\n",
    "\n",
    "price_df = price_df.sort_values(markdown_level+[price_start_date])\n",
    "price_df = price_df.groupby(markdown_level, group_keys=False).apply(remove_overlapping_dates).reset_index(drop=True)  # 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:o9_logger:------ 3. Combine Sales and price data ------\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"------ 3. Combine Sales and price data ------\")\n",
    "\n",
    "# merge actuals and price data (take zero filled actuals)\n",
    "merged_df = sales_df.merge(price_df,on=markdown_level,how='left')\n",
    "# filter rows so that each week has corresponding price (check for nulls)\n",
    "merged_df = merged_df[(merged_df[timecol] >= merged_df[price_start_date]) & (merged_df[timecol] <= merged_df[price_end_date])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:o9_logger:------ 4. Window Prediction------\n",
      "/var/folders/hm/ktk0fh81691ccjf3l0h99tq80000gn/T/ipykernel_67432/1457374490.py:9: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  merged_df[week_rank_max] = merged_df.groupby(markdown_level)[week_rank].transform(max)\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"------ 4. Window Prediction------\")\n",
    "# filter relevant data\n",
    "merged_df = merged_df[markdown_level +[timecol,actual,mkd_perc]].reset_index(drop=True)\n",
    "# sort data\n",
    "merged_df = merged_df.sort_values(markdown_level+[timecol]).reset_index(drop=True)\n",
    "\n",
    "#assign ranks/numbering to the product lifecycle\n",
    "merged_df[week_rank] = merged_df.groupby(markdown_level)[timecol].rank()\n",
    "merged_df[week_rank_max] = merged_df.groupby(markdown_level)[week_rank].transform(max)\n",
    "\n",
    "\n",
    "def first_positive_promotion_optimized(df):\n",
    "    # Identify the first positive 'promotion %' for each item-location combination\n",
    "    first_positive = df[df[mkd_perc] > 0].groupby(markdown_level)[week_rank].transform('min')\n",
    "\n",
    "    # Assign this value to a new column in the original DataFrame\n",
    "    # We use reindex to align with the original DataFrame's index\n",
    "    df[week_rank_mkd] = first_positive.reindex(df.index, fill_value=0)\n",
    "    \n",
    "    df[week_rank_mkd] = df.groupby(markdown_level)[week_rank_mkd].transform('max')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Identify the week in the lifecycle when markdown started\n",
    "merged_df = first_positive_promotion_optimized(merged_df)\n",
    "\n",
    "# calculate window start %\n",
    "merged_df[window] = merged_df[week_rank_mkd]/merged_df[week_rank_max]\n",
    "\n",
    "# merge MFP information for each planning item (check distribution data here)\n",
    "merged_df = merged_df.merge(itemmaster_df[[itemcol,mfp_level]].drop_duplicates(),on=itemcol,how='inner')\n",
    "\n",
    "# filter only positive markdowns and then calculate median window at MFP markdown_level\n",
    "mkd_window_df = merged_df[merged_df[window]>0][markdown_level+[mfp_level,window]].drop_duplicates().groupby(mfp_level)[window].agg(median_values='median', count_values='count').reset_index()\n",
    "\n",
    "# store the window values so that they can be used in real time forecasting\n",
    "save_metadata = {}\n",
    "save_metadata['df_w'] = mkd_window_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out those values which are equal to 1 as they are basically no markdown on the entire lifecycle\n",
    "# mkd_window_df = mkd_window_df[mkd_window_df['median_values']<1].reset_index(drop=True)\n",
    "# make the markdown as zero percentage as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:o9_logger:------ 5. Depth prediction starts------\n",
      "INFO:o9_logger:--- 5.1 data prep ---\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"------ 5. Depth prediction starts------\")\n",
    "logger.info(\"--- 5.1 data prep ---\")\n",
    "\n",
    "adj_mkd = 'adj_mkd %'\n",
    "\n",
    "# Filter the DataFrame to include only rows with mkd_perc > 0\n",
    "mkd_sales = merged_df[merged_df[mkd_perc] > 0]\n",
    "\n",
    "# Function to calculate weighted average for each group\n",
    "def weighted_avg(group):\n",
    "    if group[actual].sum() == 0:\n",
    "        return group[mkd_perc].mean()   # don't return zero if no sales occured\n",
    "    else:\n",
    "        return (group[actual] * group[mkd_perc]).sum() / group[actual].sum()\n",
    "\n",
    "# Calculate the weighted average promotion % for each item\n",
    "weighted_avg_mkd = mkd_sales.groupby(markdown_level).apply(weighted_avg).reset_index(name=adj_mkd)\n",
    "\n",
    "# Merge the new promotion % back into the original DataFrame\n",
    "merged_df = merged_df.merge(weighted_avg_mkd, on=markdown_level, how='left')\n",
    "\n",
    "# Fill NaN values in the new promotion % column with 0 (for items with no markdown)\n",
    "merged_df[adj_mkd].fillna(0, inplace=True)\n",
    "\n",
    "# create dataset for depth prediction\n",
    "mkd_depth_df = merged_df[markdown_level+[adj_mkd]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# merge item master\n",
    "mkd_depth_df = mkd_depth_df.merge(itemmaster_df, on = itemcol, how='left')\n",
    "\n",
    "# perform some cleaning\n",
    "mkd_depth_df.replace(\"UNDEFINED\", np.nan, inplace=True)\n",
    "mkd_depth_df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:o9_logger:--- 5.2 train and test data ---\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- 5.2 train and test data ---\")\n",
    "\n",
    "# Split into training and test sets\n",
    "X = mkd_depth_df.drop(columns=[adj_mkd,itemcol,versioncol,is_assorted, actual]) \n",
    "y = mkd_depth_df[adj_mkd]\n",
    "\n",
    "save_metadata['train_columns']=X.columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=10)\n",
    "\n",
    "X_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "X_test = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "\n",
    "cat_features=X_train.dtypes[X_train.dtypes == 'object'].index.to_list()\n",
    "for c in cat_features:\n",
    "    X_train[c] = X_train[c].astype('category')\n",
    "    X_test[c] = X_test[c].astype('category')\n",
    "\n",
    "save_metadata['cat_columns'] = cat_features\n",
    "\n",
    "# Create LightGBM dataset\n",
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature = cat_features)\n",
    "lgb_test = lgb.Dataset(X_test, y_test, categorical_feature = cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashvardhansingh/miniconda3/envs/yashml/lib/python3.11/site-packages/lightgbm/engine.py:172: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1041\n",
      "[LightGBM] [Info] Number of data points in the train set: 144749, number of used features: 43\n",
      "[LightGBM] [Info] Start training from score -1.436190\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's tweedie: 1.92069\n",
      "[100]\tvalid_0's tweedie: 1.91037\n",
      "[150]\tvalid_0's tweedie: 1.90455\n",
      "[200]\tvalid_0's tweedie: 1.90044\n",
      "[250]\tvalid_0's tweedie: 1.89727\n",
      "[300]\tvalid_0's tweedie: 1.89466\n",
      "[350]\tvalid_0's tweedie: 1.89248\n",
      "[400]\tvalid_0's tweedie: 1.8907\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[450]\tvalid_0's tweedie: 1.88898\n",
      "[500]\tvalid_0's tweedie: 1.88753\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[550]\tvalid_0's tweedie: 1.88602\n",
      "[600]\tvalid_0's tweedie: 1.88455\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[650]\tvalid_0's tweedie: 1.88315\n",
      "[700]\tvalid_0's tweedie: 1.88169\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[750]\tvalid_0's tweedie: 1.88026\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[800]\tvalid_0's tweedie: 1.87871\n",
      "[850]\tvalid_0's tweedie: 1.87732\n",
      "[900]\tvalid_0's tweedie: 1.87602\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[950]\tvalid_0's tweedie: 1.8749\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1000]\tvalid_0's tweedie: 1.87358\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1050]\tvalid_0's tweedie: 1.87251\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1100]\tvalid_0's tweedie: 1.87147\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1150]\tvalid_0's tweedie: 1.87041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1200]\tvalid_0's tweedie: 1.86943\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1250]\tvalid_0's tweedie: 1.86845\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1300]\tvalid_0's tweedie: 1.8676\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1350]\tvalid_0's tweedie: 1.86666\n",
      "[1400]\tvalid_0's tweedie: 1.86557\n",
      "[1450]\tvalid_0's tweedie: 1.86427\n",
      "[1500]\tvalid_0's tweedie: 1.86345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1550]\tvalid_0's tweedie: 1.86266\n",
      "[1600]\tvalid_0's tweedie: 1.86174\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1650]\tvalid_0's tweedie: 1.86104\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1700]\tvalid_0's tweedie: 1.86044\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1750]\tvalid_0's tweedie: 1.85993\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1800]\tvalid_0's tweedie: 1.85934\n",
      "[1850]\tvalid_0's tweedie: 1.85865\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1900]\tvalid_0's tweedie: 1.85806\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1950]\tvalid_0's tweedie: 1.85732\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2000]\tvalid_0's tweedie: 1.85672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2050]\tvalid_0's tweedie: 1.85624\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2100]\tvalid_0's tweedie: 1.85574\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2150]\tvalid_0's tweedie: 1.85516\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2200]\tvalid_0's tweedie: 1.85452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2250]\tvalid_0's tweedie: 1.85394\n",
      "[2300]\tvalid_0's tweedie: 1.85331\n",
      "[2350]\tvalid_0's tweedie: 1.85277\n",
      "[2400]\tvalid_0's tweedie: 1.85217\n",
      "[2450]\tvalid_0's tweedie: 1.85143\n",
      "[2500]\tvalid_0's tweedie: 1.85078\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2550]\tvalid_0's tweedie: 1.85025\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2600]\tvalid_0's tweedie: 1.84982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2650]\tvalid_0's tweedie: 1.84931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2700]\tvalid_0's tweedie: 1.84889\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2750]\tvalid_0's tweedie: 1.84849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2800]\tvalid_0's tweedie: 1.84807\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2850]\tvalid_0's tweedie: 1.84756\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2900]\tvalid_0's tweedie: 1.84689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2950]\tvalid_0's tweedie: 1.84646\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3000]\tvalid_0's tweedie: 1.84586\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3050]\tvalid_0's tweedie: 1.84528\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3100]\tvalid_0's tweedie: 1.84482\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3150]\tvalid_0's tweedie: 1.84434\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3200]\tvalid_0's tweedie: 1.84375\n",
      "[3250]\tvalid_0's tweedie: 1.84316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3300]\tvalid_0's tweedie: 1.84272\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3350]\tvalid_0's tweedie: 1.84224\n",
      "[3400]\tvalid_0's tweedie: 1.84181\n",
      "[3450]\tvalid_0's tweedie: 1.84136\n",
      "[3500]\tvalid_0's tweedie: 1.84088\n",
      "[3550]\tvalid_0's tweedie: 1.84017\n",
      "[3600]\tvalid_0's tweedie: 1.8397\n",
      "[3650]\tvalid_0's tweedie: 1.8393\n",
      "[3700]\tvalid_0's tweedie: 1.83886\n",
      "[3750]\tvalid_0's tweedie: 1.8384\n",
      "[3800]\tvalid_0's tweedie: 1.83796\n",
      "[3850]\tvalid_0's tweedie: 1.83755\n",
      "[3900]\tvalid_0's tweedie: 1.83715\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3950]\tvalid_0's tweedie: 1.83681\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4000]\tvalid_0's tweedie: 1.83648\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4000]\tvalid_0's tweedie: 1.83648\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- 5.3 ML training ---\")\n",
    "\n",
    "# Set LightGBM hyperparameters\n",
    "params = {\n",
    "    'objective': 'tweedie',  # regression gamma mape poisson tweedie   (mape,mape)\n",
    "    'metric': '',  # rmse mape\n",
    "    'min_data_in_leaf':25,\n",
    "    'learning_rate': 0.02, #0.05 0.02\n",
    "    'num_iterations' : 4000, \n",
    "    'num_leaves' : 50,\n",
    "    \"max_depth\" : 8,\n",
    "    \"n_jobs\" : 5,\n",
    "}\n",
    "\n",
    "# 'tweedie_variance_power': 1.5\n",
    "\n",
    "# Train LightGBM model\n",
    "model = lgb.train(params, lgb_train,categorical_feature = cat_features,valid_sets=[lgb_test],callbacks=[early_stopping(50), log_evaluation(50)])\n",
    "\n",
    "# %%\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_compare_lgbm = y_test.to_frame().copy()\n",
    "y_compare_lgbm['pred'] = y_pred\n",
    "\n",
    "y_pred[y_pred < 0.1] = 0\n",
    "\n",
    "\n",
    "# Get accuracy on test set\n",
    "rmse_accuracy = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "logger.info('Test RMSE: {}'.format(rmse_accuracy))\n",
    "\n",
    "#lgb.plot_importance(model,height = 0.5,dpi=200,figsize=(15,15),importance_type='gain')\n",
    "#plot_regression_accuracy(y_test,y_pred,1,1)\n",
    "\n",
    "# Calculate feature importance\n",
    "#feature_importance = model.feature_importance(importance_type='gain')\n",
    "\n",
    "# Normalize the feature importance scores\n",
    "#normalized_importance = feature_importance / np.sum(feature_importance)\n",
    "\n",
    "#MLimportance = pd.DataFrame({\"Features\":model.feature_name(),\"Importance\":normalized_importance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- 5.4 Get Prediction data from assortment ---\")\n",
    "\n",
    "mkd_output_level = [itemcol,'Location.[Location]']\n",
    "\n",
    "## process and filter assortment data\n",
    "assortment_df[assortment_start_date] = pd.to_datetime(assortment_df[assortment_start_date], format='%m/%d/%Y %I:%M:%S %p')\n",
    "assortment_df[assortment_end_date] = pd.to_datetime(assortment_df[assortment_end_date], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "## filter the data\n",
    "date_3_years_later = current_week + pd.DateOffset(years=3)\n",
    "date_x_weeks_back = current_week + pd.DateOffset(weeks=-26)\n",
    "\n",
    "## filter assortment_df for Pre-season  and inseason mkdn prediction\n",
    "date_3_years_later = current_week + pd.DateOffset(years=3)\n",
    "date_x_weeks_back = current_week + pd.DateOffset(weeks=-26)\n",
    "\n",
    "assortment_df_preseason = assortment_df.loc[(assortment_df['EB_Assorted']==True)&(assortment_df['EB_NoMarkDown']==False)&\n",
    "                                       (assortment_df[assortment_end_date] < date_3_years_later)&\n",
    "                                       (assortment_df[assortment_start_date] > date_x_weeks_back),:].reset_index(drop=True)\n",
    "\n",
    "## filter assortment_df for old stock markdown generation\n",
    "date_x_weeks_back = current_week + pd.DateOffset(weeks=-80)  # go back around 1.5 years\n",
    "\n",
    "assortment_df_old_stock = assortment_df.loc[(assortment_df['EB_Assorted']==False)&(assortment_df['EB_NoMarkDown']==False)&\n",
    "                                       (assortment_df[assortment_end_date] < current_week)&\n",
    "                                       (assortment_df[assortment_start_date] > date_x_weeks_back),:].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- 5.5 Preseason markdown Prediction ---\")\n",
    "\n",
    "## get location info - country\n",
    "assortment_df_preseason = assortment_df_preseason.merge(location_master[['Location.[Location]', 'Location.[Location Country]']],on='Location.[Location]',how='left')\n",
    "# get itemmaster info\n",
    "assortment_df_preseason = assortment_df_preseason.merge(itemmaster_df.drop(['Version.[Version Name]','EB_Assorted', 'Actual'],axis=1), on = itemcol, how='left')\n",
    "# drop unnecessary columns\n",
    "assortment_df_preseason = assortment_df_preseason.drop(['Version.[Version Name]','EB_Season.[EB_Season]', 'EB_Item_Seasonality.[EB_Item_Seasonality]'],axis=1)\n",
    "\n",
    "# remove duplicates created in the assortment data due to season dimension\n",
    "assortment_df_preseason = assortment_df_preseason.drop_duplicates(subset = mkd_output_level+ [assortment_start_date]).reset_index(drop=True)\n",
    "\n",
    "# perform some cleaning\n",
    "assortment_df_preseason.replace(\"UNDEFINED\", np.nan, inplace=True)\n",
    "\n",
    "## perform mkd_perc % prediction\n",
    "dfassort_predict = assortment_df_preseason.copy().rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "common_columns = dfassort_predict.columns.intersection(X_train.columns)\n",
    "for c in cat_features:\n",
    "    dfassort_predict[c] = dfassort_predict[c].astype('category')\n",
    "assortment_df_preseason[adj_mkd] = model.predict(dfassort_predict[common_columns])\n",
    "\n",
    "# adjust the mkd_perc % values\n",
    "assortment_df_preseason.loc[assortment_df_preseason[adj_mkd]<0.1,adj_mkd] = 0\n",
    "assortment_df_preseason[adj_mkd] = (assortment_df_preseason[adj_mkd].round(2) / 0.05).round() * 0.05\n",
    "\n",
    "# get the window values and fill zero where no present\n",
    "assortment_df_preseason = assortment_df_preseason.merge(mkd_window_df[[mfp_level,median_window]],on = mfp_level,how='left')\n",
    "assortment_df_preseason[median_window].fillna(0,inplace=True)\n",
    "\n",
    "## filter the combinations for which output will be given and relevent columns\n",
    "preseason_mkd_output = assortment_df_preseason[(assortment_df_preseason[median_window]>0) & \n",
    "                                               (assortment_df_preseason[adj_mkd]>0)][mkd_output_level+\n",
    "                                                                                     [assortment_start_date,assortment_end_date,adj_mkd,\n",
    "                                                                                      median_window]].reset_index(drop=True)\n",
    "\n",
    "# adjust the start and end dates to the start of the week\n",
    "preseason_mkd_output[assortment_start_date] = preseason_mkd_output[assortment_start_date] - pd.to_timedelta(preseason_mkd_output[assortment_start_date].dt.weekday, unit='d')\n",
    "preseason_mkd_output[assortment_end_date] = preseason_mkd_output[assortment_end_date] - pd.to_timedelta(preseason_mkd_output[assortment_end_date].dt.weekday, unit='d')\n",
    "\n",
    "# Calculate the start_date based on the window percentage\n",
    "preseason_mkd_output['new_start_date'] = preseason_mkd_output[assortment_end_date] - pd.to_timedelta((1-preseason_mkd_output[median_window]) * (preseason_mkd_output[assortment_end_date] - preseason_mkd_output[assortment_start_date]).dt.days, unit='D')\n",
    "preseason_mkd_output['new_start_date'] = preseason_mkd_output['new_start_date'] - pd.to_timedelta(preseason_mkd_output['new_start_date'].dt.weekday, unit='d')\n",
    "preseason_mkd_output['new_start_date'] = preseason_mkd_output['new_start_date'].dt.floor('D')\n",
    "\n",
    "# explode the dates between the new start and end dates\n",
    "date_ranges = (\n",
    "    preseason_mkd_output.apply(lambda row: pd.date_range(start=row['new_start_date'], end=row[assortment_end_date], freq='W-MON'), axis=1)\n",
    "    .explode()\n",
    ")\n",
    "\n",
    "#merge them with the original dataframe and rename the columns\n",
    "preseason_mkd_output = pd.merge(preseason_mkd_output[mkd_output_level+[adj_mkd]], date_ranges.to_frame(), left_index=True, right_index=True)\n",
    "preseason_mkd_output = preseason_mkd_output.rename(columns={0:timecol,adj_mkd:'EB_AvgMarkdownPriceLB'}).reset_index(drop=True)\n",
    "\n",
    "#get markdown values that are greater than the current date\n",
    "preseason_mkd_output = preseason_mkd_output[preseason_mkd_output[timecol]>current_week].reset_index(drop=True)\n",
    "\n",
    "# get the tenant format of dates from timemaster\n",
    "dimtime[timecol] = pd.to_datetime(dimtime[timecol])\n",
    "preseason_mkd_output = preseason_mkd_output.merge(dimtime,on=timecol,how='left').drop(timecol,axis=1)\n",
    "\n",
    "# add version col\n",
    "preseason_mkd_output[versioncol] = input_version\n",
    "\n",
    "# add season infromation to the data provided \n",
    "season_merge_level = [itemcol,'Time.[Week]']\n",
    "preseason_mkd_output = preseason_mkd_output.merge(itemseasonweek[season_merge_level+[item_season]].drop_duplicates(),on=season_merge_level,how='left')\n",
    "\n",
    "# drop rows whereever season information is not present\n",
    "preseason_mkd_output = preseason_mkd_output.dropna()\n",
    "\n",
    "#filter relevant rows for output\n",
    "preseason_mkd_output = preseason_mkd_output[[versioncol] + mkd_output_level + ['Time.[Week]',item_season,'EB_AvgMarkdownPriceLB']]\n",
    "\n",
    "logger.info(\"preseason_mkd_output head:{}\".format(preseason_mkd_output.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- 5.6 Old Stock markdown Prediction ---\")\n",
    "\n",
    "old_stock_mkdn_output = merged_df[(merged_df['weekrank']==merged_df['week_rank_max'])&\n",
    "                                  (merged_df['markdown_perc']>0)][markdown_level+ [timecol,mkd_perc]].reset_index(drop=True)\n",
    "\n",
    "## get location info - country for assortment old stock\n",
    "assortment_df_old_stock = assortment_df_old_stock.merge(location_master[['Location.[Location]', 'Location.[Location Country]']],on='Location.[Location]',how='left')\n",
    "\n",
    "old_stock_mkdn_output = old_stock_mkdn_output.merge(assortment_df_old_stock[mkd_output_level + [loccol]].drop_duplicates(),on=markdown_level,how='inner')\n",
    "\n",
    "# add tenant format week inforation\n",
    "old_stock_mkdn_output = old_stock_mkdn_output.merge(dimtime,on=timecol,how='left').drop(timecol,axis=1)\n",
    "\n",
    "# add version col\n",
    "old_stock_mkdn_output[versioncol] = input_version\n",
    "\n",
    "# add season infromation to the data provided \n",
    "season_merge_level = [itemcol,'Time.[Week]']\n",
    "old_stock_mkdn_output = old_stock_mkdn_output.merge(itemseasonweek[season_merge_level+[item_season]].drop_duplicates(),on=season_merge_level,how='left')\n",
    "\n",
    "# drop rows whereever season information is not present\n",
    "old_stock_mkdn_output = old_stock_mkdn_output.dropna()\n",
    "\n",
    "\n",
    "#filter relevant rows for output\n",
    "old_stock_mkdn_output = old_stock_mkdn_output[[versioncol] + mkd_output_level + \n",
    "                                              [item_season,mkd_perc]].drop_duplicates().reset_index(drop=True).rename(columns={mkd_perc:'EB_MFP_OldStock_Markdown_Percent'})\n",
    "\n",
    "# add O- to the season column to  show they belong to an older season\n",
    "old_stock_mkdn_output[item_season] = 'O-' + old_stock_mkdn_output[item_season]\n",
    "\n",
    "logger.info(\"old_stock_mkdn_output head:{}\".format(old_stock_mkdn_output.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model to cloud \n",
    "logger.info(\"----- 6.1 save model to cloud  -----\")\n",
    "bucket_name = 'stgte00012file-eu'\n",
    "\n",
    "## TODO: Change the model file name\n",
    "model_name = \"Pre_season_markdown_lgbm_model.txt\"\n",
    "model.save_model(model_name)     # to load model bst = lgb.Booster(model_file='model.txt')\n",
    "bucket = storage.Client().bucket(bucket_name)\n",
    "## TODO: Change the GCS folder name for model in GCS\n",
    "model_path = \"Pre_season_markdown_models/\"+model_name\n",
    "blob = bucket.blob(model_path)\n",
    "blob.upload_from_filename(model_name)\n",
    "logger.info(\"Url for saved blobs: {}\".format(blob))\n",
    "os.remove(model_name)\n",
    "\n",
    "logger.info(\"----- 6.2 save metadata to cloud  -----\")\n",
    "      \n",
    "## TODO: Change the metadata file name\n",
    "file_name = 'Pre_season_markdown_metadata.json'\n",
    "with open(file_name, 'w') as outfile:\n",
    "    json.dump(save_metadata, outfile)\n",
    "bucket = storage.Client().bucket(bucket_name)\n",
    "model_path = \"Pre_season_markdown_models/\"+file_name\n",
    "blob = bucket.blob(model_path)\n",
    "blob.upload_from_filename(file_name)\n",
    "logger.info(\"Url for saved blobs: {}\".format(blob))\n",
    "os.remove(file_name) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
